{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HSE 2021: Mathematical Methods for Data Analysis\n",
    "\n",
    "## Homework 2\n",
    "\n",
    "### Attention!\n",
    "* For tasks where <ins>text answer</ins> is required **Russian language** is **allowed**.\n",
    "* If a task asks you to describe something (make coclusions) then **text answer** is **mandatory** and **is** part of the task\n",
    "* We **only** accept **ipynb** notebooks. If you use Google Colab then you'll have to download the notebook before passing the homework\n",
    "* **Do not** use python loops instead of NumPy vector operations over NumPy vectors - it significantly decreases performance (see why https://blog.paperspace.com/numpy-optimization-vectorization-and-broadcasting/), will be punished with -0.25 for **every** task. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-26T16:48:20.566549Z",
     "start_time": "2020-09-26T16:48:19.893995Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "from sklearn import datasets\n",
    "from sklearn.datasets import load_boston\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "sns.set(style=\"darkgrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n",
    "\n",
    "For this homework we use Boston Dataset from sklearn (based on UCI ML housing dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function load_boston is deprecated; `load_boston` is deprecated in 1.0 and will be removed in 1.2.\n",
      "\n",
      "    The Boston housing prices dataset has an ethical problem. You can refer to\n",
      "    the documentation of this function for further details.\n",
      "\n",
      "    The scikit-learn maintainers therefore strongly discourage the use of this\n",
      "    dataset unless the purpose of the code is to study and educate about\n",
      "    ethical issues in data science and machine learning.\n",
      "\n",
      "    In this case special case, you can fetch the dataset from the original\n",
      "    source::\n",
      "\n",
      "        import pandas as pd\n",
      "        import numpy as np\n",
      "\n",
      "\n",
      "        data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
      "        raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
      "        data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
      "        target = raw_df.values[1::2, 2]\n",
      "\n",
      "    Alternative datasets include the California housing dataset (i.e.\n",
      "    func:`~sklearn.datasets.fetch_california_housing`) and the Ames housing\n",
      "    dataset. You can load the datasets as follows:\n",
      "\n",
      "        from sklearn.datasets import fetch_california_housing\n",
      "        housing = fetch_california_housing()\n",
      "\n",
      "    for the California housing dataset and:\n",
      "\n",
      "        from sklearn.datasets import fetch_openml\n",
      "        housing = fetch_openml(name=\"house_prices\", as_frame=True)\n",
      "\n",
      "    for the Ames housing dataset.\n",
      "    \n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "data = load_boston() # load dataset\n",
    "\n",
    "data_X = data.data\n",
    "data_Y = data.target\n",
    "data_Columns = data.feature_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. [0.5 points] \n",
    "Create Pandas DataFrame and split the data into train and test sets with ratio 80:20 with random_state=0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Splitting\n",
    "X_train, X_test, y_train, y_test = train_test_split(data_X, data_Y, train_size=.8, random_state=0)\n",
    "\n",
    "# Normalization\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.fit_transform(X_test)\n",
    "\n",
    "# Creating data frames\n",
    "X_train = pd.DataFrame(data=X_train, columns=data_Columns, index=np.arange(len(X_train)))\n",
    "X_test = pd.DataFrame(data=X_test, columns=data_Columns, index=np.arange(len(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### 2. [1 point] \n",
    "Train models on train data using StatsModels( or sckit-learn) library and apply it to the test set; use $RMSE$ and $R^2$ as the quality measure.\n",
    "\n",
    "* [`LinearRegression`](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html);\n",
    "* [`Ridge`](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html) with $\\alpha = 0.01$;\n",
    "* [`Lasso`](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html) with $\\alpha = 0.01$\n",
    "\n",
    "Don't forget to scale the data before training the models with StandardScaler!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearRegression() rmse score = 5.926; r2 score = 0.569\n",
      "Ridge(alpha=0.01) rmse score = 5.926; r2 score = 0.569\n",
      "Lasso(alpha=0.01) rmse score = 5.937; r2 score = 0.567\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "\n",
    "clsfs = [\n",
    "    LinearRegression(),\n",
    "    Ridge(alpha=0.01),\n",
    "    Lasso(alpha=0.01)\n",
    "]\n",
    "\n",
    "for clsf in clsfs:\n",
    "    clsf.fit(X_train, y_train)\n",
    "    y = clsf.predict(X_test)\n",
    "    print(f\"{str(clsf)} rmse score = {mean_squared_error(y_test, y, squared=False):.3f}\"\n",
    "    + f\"; r2 score = {r2_score(y_test, y):.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### 3. [1 point] \n",
    "Explore the values of the parameters of the resulting models and compare the number of zero weights in them. \n",
    "\n",
    "Comment on the significance of the coefficients, overal model significance and other related factors from the results table. \n",
    "\n",
    "`Hint` Use StatModels to obtain significance of the coefficients. They ca be found on the `summary` of the fitted linear model. \n",
    "It might be tricky to obtain `summary` for the regularized model. Please, read the documentation of the StatModels library to figure out how to do that, e.g.   [OLSResults](https://www.statsmodels.org/stable/generated/statsmodels.regression.linear_model.OLSResults.html#statsmodels.regression.linear_model.OLSResults) class might be useful here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients for LinearRegression()\n",
      "intercept = 22.611881188118836\n",
      "LSTAT -> -3.592\n",
      "DIS -> -2.881\n",
      "RM -> 2.573\n",
      "PTRATIO -> -2.293\n",
      "RAD -> 2.112\n",
      "TAX -> -1.875\n",
      "NOX -> -1.855\n",
      "ZN -> 1.057\n",
      "CRIM -> -0.971\n",
      "B -> 0.718\n",
      "CHAS -> 0.595\n",
      "AGE -> -0.088\n",
      "INDUS -> 0.038\n",
      "\n",
      "Coefficients for Ridge(alpha=0.01)\n",
      "intercept = 22.611881188118836\n",
      "LSTAT -> -3.592\n",
      "DIS -> -2.881\n",
      "RM -> 2.573\n",
      "PTRATIO -> -2.293\n",
      "RAD -> 2.111\n",
      "TAX -> -1.875\n",
      "NOX -> -1.855\n",
      "ZN -> 1.057\n",
      "CRIM -> -0.971\n",
      "B -> 0.718\n",
      "CHAS -> 0.595\n",
      "AGE -> -0.088\n",
      "INDUS -> 0.038\n",
      "\n",
      "Coefficients for Lasso(alpha=0.01)\n",
      "intercept = 22.611881188118836\n",
      "LSTAT -> -3.596\n",
      "DIS -> -2.809\n",
      "RM -> 2.585\n",
      "PTRATIO -> -2.279\n",
      "RAD -> 1.956\n",
      "NOX -> -1.804\n",
      "TAX -> -1.738\n",
      "ZN -> 1.022\n",
      "CRIM -> -0.940\n",
      "B -> 0.705\n",
      "CHAS -> 0.595\n",
      "AGE -> -0.069\n",
      "INDUS -> -0.000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for clsf in clsfs:\n",
    "    arr = clsf.coef_\n",
    "    print(f'Coefficients for {str(clsf)}')\n",
    "    print(f\"intercept = {clsf.intercept_}\")\n",
    "    # In order of importance\n",
    "    for i in np.argsort(-np.abs(arr)):\n",
    "        print(f'{data_Columns[i]} -> {arr[i]:.3f}')\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Conclusion```\n",
    "\n",
    "There are two almost-zero parameters: AGE and INDUS\n",
    "\n",
    "Most coefficients are significant (abs is >1 with intercept = 22.6) so most features are relevant to the model\n",
    "\n",
    "About 57% of the variance in target is explained by the model (R2 score) which is pretty decent.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### 4. [1 point] \n",
    "Implement one of the elimination algorithms that were described in the Seminar_4 (Elimination by P-value, Forward elimination, Backward elimination), make conclusions. \n",
    "It's enough to apply to one of the models above (e.g simple linear regression)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backward elimination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "data_X = scaler.fit_transform(data_X)\n",
    "data_X = pd.DataFrame(data=data_X, columns=data_Columns, index=np.arange(len(data_X)))\n",
    "\n",
    "y = data_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "eliminated = []\n",
    "features = list(data_Columns)\n",
    "loss_history = []\n",
    "\n",
    "while len(features) > 1:\n",
    "    X = data_X[features]\n",
    "    X = sm.add_constant(X)\n",
    "    model = sm.OLS(y, X)\n",
    "    res = model.fit()\n",
    "    s = res.summary2()\n",
    "    t = s.tables[1]['P>|t|']\n",
    "    \n",
    "    worst_index = t.argmax() - 1\n",
    "    \n",
    "    # R2-adjusted\n",
    "    score = float(s.tables[0][1][6])\n",
    "    \n",
    "    if len(loss_history) and loss_history[-1] > score:\n",
    "        break\n",
    "    else:\n",
    "        loss_history.append( score )\n",
    "        eliminated.append(features[worst_index])\n",
    "        del features[worst_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BarContainer object of 3 artists>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "loss_history=np.array(loss_history)\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.bar(eliminated, loss_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CRIM', 'ZN', 'NOX', 'RM', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Conclusion```\n",
    "\n",
    "The score was best before deleting the feature 'CHAS' so the remaining features are:\n",
    "\n",
    "'CRIM', 'ZN', 'NOX', 'RM', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### 5. [1 point] \n",
    "Find the best (in terms of RMSE) $\\alpha$ for Ridge regression using cross-validation with 5 folds. You must select values from range $[10^{-4}, 10^{3}]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([0.00229735, 0.00209637, 0.00179873, 0.0020968 , 0.00180521,\n",
       "        0.00199256, 0.00199995, 0.00199804, 0.00239763, 0.00210128,\n",
       "        0.00209556, 0.00199304, 0.00209637, 0.0017992 , 0.00180626,\n",
       "        0.00199885, 0.00199881, 0.00179391, 0.00220518, 0.00239806,\n",
       "        0.00339427, 0.00229573, 0.00201054, 0.00219779, 0.00189605,\n",
       "        0.00199385, 0.00220175, 0.00180521, 0.0023982 , 0.00229673,\n",
       "        0.00200586, 0.00249705, 0.00219855, 0.00220466, 0.0017931 ,\n",
       "        0.00210242, 0.00219674, 0.00200486, 0.00169659, 0.00200443,\n",
       "        0.00200305, 0.00209594, 0.00139856, 0.00150142, 0.00220366,\n",
       "        0.00279255, 0.00239973, 0.00239649, 0.00219421, 0.00199456,\n",
       "        0.00139613, 0.00199447, 0.0017952 , 0.00099745, 0.00219431,\n",
       "        0.00319118, 0.00159574, 0.00219431, 0.00219417, 0.00199456,\n",
       "        0.00239363, 0.00418849, 0.00199461, 0.00179539, 0.00199471,\n",
       "        0.00179524, 0.00159569, 0.0017952 , 0.00119677, 0.0009974 ,\n",
       "        0.0017952 , 0.0009973 , 0.00159559, 0.00179501, 0.00339093,\n",
       "        0.0025929 , 0.00219417, 0.00209622, 0.00219436, 0.00219388,\n",
       "        0.00199428, 0.00179534, 0.00139623, 0.0009975 , 0.00159569,\n",
       "        0.00139594, 0.00139632, 0.00199485, 0.00219431, 0.0017942 ,\n",
       "        0.00219431, 0.00199447, 0.0023932 , 0.00219383, 0.0017952 ,\n",
       "        0.00159554, 0.00199466, 0.00159559, 0.00119672, 0.00139632]),\n",
       " 'std_fit_time': array([4.00397195e-04, 2.07585864e-04, 4.01133194e-04, 2.06649947e-04,\n",
       "        4.03968651e-04, 4.58260108e-06, 3.19508067e-04, 1.23321189e-05,\n",
       "        5.80340317e-04, 2.01042180e-04, 4.87529993e-04, 6.30989778e-04,\n",
       "        2.05671281e-04, 4.04539793e-04, 4.04665938e-04, 1.01179407e-05,\n",
       "        6.35749900e-04, 3.97479666e-04, 4.08159110e-04, 4.94768345e-04,\n",
       "        2.78824699e-03, 4.00173295e-04, 2.18115345e-05, 3.97423745e-04,\n",
       "        1.96648685e-04, 2.39560255e-06, 5.10509072e-04, 4.04462559e-04,\n",
       "        4.82343316e-04, 4.02155351e-04, 1.32903336e-05, 7.72637548e-04,\n",
       "        2.50180448e-04, 4.09806099e-04, 7.46789582e-04, 4.93117393e-04,\n",
       "        3.97985141e-04, 6.31095249e-04, 3.98282786e-04, 1.25865140e-05,\n",
       "        1.04606814e-05, 4.87159161e-04, 3.73856845e-04, 4.51445100e-04,\n",
       "        3.94392455e-04, 7.46531348e-04, 7.95190484e-04, 8.07096222e-04,\n",
       "        3.99208325e-04, 6.31203440e-04, 4.88850176e-04, 2.43140197e-07,\n",
       "        3.99065146e-04, 3.23406696e-07, 3.99398832e-04, 2.91750084e-03,\n",
       "        4.88714100e-04, 7.46506239e-04, 3.99232605e-04, 6.64157308e-07,\n",
       "        4.89084102e-04, 3.90875254e-03, 6.31128178e-04, 3.98922166e-04,\n",
       "        4.42200589e-07, 3.99089127e-04, 4.88383019e-04, 3.98945876e-04,\n",
       "        3.99041215e-04, 4.67203091e-07, 3.99065089e-04, 2.13248060e-07,\n",
       "        4.88597253e-04, 3.99089241e-04, 1.85006455e-03, 4.89278763e-04,\n",
       "        9.77525281e-04, 4.87005824e-04, 7.46493321e-04, 3.99136981e-04,\n",
       "        1.90734863e-07, 3.98898249e-04, 4.88675027e-04, 2.33601546e-07,\n",
       "        4.88577679e-04, 4.88519261e-04, 4.88694643e-04, 6.31052662e-04,\n",
       "        3.99398946e-04, 7.48229488e-04, 7.46697006e-04, 6.31052673e-04,\n",
       "        4.89045115e-04, 3.99518109e-04, 3.99065032e-04, 4.88460902e-04,\n",
       "        1.78416128e-07, 4.88597113e-04, 3.98945819e-04, 4.88597160e-04]),\n",
       " 'mean_score_time': array([0.00149779, 0.00140095, 0.00180054, 0.00149708, 0.00119586,\n",
       "        0.0013072 , 0.00169702, 0.00140238, 0.00149789, 0.00109677,\n",
       "        0.00160069, 0.00150404, 0.00149703, 0.00189705, 0.00139313,\n",
       "        0.00180097, 0.0016995 , 0.00120997, 0.00119596, 0.001197  ,\n",
       "        0.00120955, 0.00140057, 0.00158739, 0.00120301, 0.00150518,\n",
       "        0.00120225, 0.00109854, 0.00119586, 0.0014009 , 0.00110583,\n",
       "        0.0011961 , 0.00170145, 0.00169778, 0.0013957 , 0.00150447,\n",
       "        0.00090046, 0.00160251, 0.00159388, 0.00139608, 0.0011951 ,\n",
       "        0.00119576, 0.00119653, 0.00149994, 0.00099759, 0.00159411,\n",
       "        0.00180573, 0.00199838, 0.0016006 , 0.00199456, 0.00119686,\n",
       "        0.0009974 , 0.0009975 , 0.00099735, 0.00139623, 0.0009973 ,\n",
       "        0.00119686, 0.00119715, 0.00179491, 0.00139642, 0.00139642,\n",
       "        0.00139608, 0.00239382, 0.00179534, 0.00119677, 0.0009974 ,\n",
       "        0.00119677, 0.0009975 , 0.00099721, 0.00119686, 0.00159578,\n",
       "        0.00099754, 0.00099745, 0.00099783, 0.0011972 , 0.00139627,\n",
       "        0.00159593, 0.00139627, 0.00159564, 0.00119658, 0.0011971 ,\n",
       "        0.00099773, 0.00099721, 0.00119686, 0.0009973 , 0.00119686,\n",
       "        0.00099773, 0.00119686, 0.00139623, 0.00139632, 0.0015955 ,\n",
       "        0.00139608, 0.001197  , 0.00139651, 0.00119681, 0.00119672,\n",
       "        0.00119696, 0.00099735, 0.0009975 , 0.00119686, 0.00119681]),\n",
       " 'std_score_time': array([4.46234837e-04, 3.74329287e-04, 4.01748733e-04, 4.45534998e-04,\n",
       "        3.99617914e-04, 3.96106805e-04, 3.98313567e-04, 4.95211471e-04,\n",
       "        4.46552528e-04, 2.02414784e-04, 4.92451492e-04, 6.39236743e-04,\n",
       "        4.45748794e-04, 6.59363629e-04, 4.88007304e-04, 4.01985586e-04,\n",
       "        3.97352338e-04, 3.92400739e-04, 3.99453901e-04, 3.98564406e-04,\n",
       "        4.06329906e-04, 5.87256770e-04, 4.84324771e-04, 4.10700107e-04,\n",
       "        4.53048286e-04, 3.96079572e-04, 2.02107844e-04, 3.99619707e-04,\n",
       "        4.94717515e-04, 4.94441585e-04, 3.99621357e-04, 4.02136359e-04,\n",
       "        3.98294603e-04, 4.89305213e-04, 4.51639994e-04, 1.93811189e-04,\n",
       "        4.80443113e-04, 4.87104069e-04, 4.88402507e-04, 3.95345760e-04,\n",
       "        3.96442442e-04, 3.98802825e-04, 4.46229104e-04, 2.78041453e-07,\n",
       "        4.90021756e-04, 7.49434259e-04, 8.99096439e-04, 8.00408514e-04,\n",
       "        8.92176842e-04, 3.98874354e-04, 3.56832255e-07, 3.50402318e-07,\n",
       "        2.78041453e-07, 4.88480335e-04, 3.01578299e-07, 3.98874354e-04,\n",
       "        3.99089241e-04, 3.98802996e-04, 4.89298080e-04, 4.88908628e-04,\n",
       "        4.88305338e-04, 1.01731738e-03, 7.46875528e-04, 3.99160542e-04,\n",
       "        3.87384339e-07, 3.99041244e-04, 3.81469727e-07, 2.43140197e-07,\n",
       "        3.98874297e-04, 4.88655694e-04, 5.00111031e-07, 3.56832255e-07,\n",
       "        2.78041453e-07, 3.99065431e-04, 4.88928267e-04, 4.88869943e-04,\n",
       "        4.88636574e-04, 4.89025522e-04, 3.98898249e-04, 3.99351163e-04,\n",
       "        1.78416128e-07, 1.16800773e-07, 3.99112730e-04, 3.37174788e-07,\n",
       "        3.98874297e-04, 9.53674316e-08, 3.98755198e-04, 4.88480429e-04,\n",
       "        4.89084009e-04, 7.98404239e-04, 4.89083777e-04, 3.99160542e-04,\n",
       "        4.89025382e-04, 3.98778930e-04, 3.98945876e-04, 3.98826628e-04,\n",
       "        1.78416128e-07, 1.78416128e-07, 3.98874297e-04, 3.98659802e-04]),\n",
       " 'param_alpha': masked_array(data=[0.0001, 0.0001176811952434998, 0.00013848863713938732,\n",
       "                    0.00016297508346206434, 0.00019179102616724886,\n",
       "                    0.0002257019719633919, 0.00026560877829466864,\n",
       "                    0.00031257158496882353, 0.0003678379771828634,\n",
       "                    0.0004328761281083057, 0.000509413801481638,\n",
       "                    0.0005994842503189409, 0.0007054802310718645,\n",
       "                    0.0008302175681319744, 0.0009770099572992247,\n",
       "                    0.0011497569953977356, 0.0013530477745798076,\n",
       "                    0.0015922827933410922, 0.001873817422860383,\n",
       "                    0.0022051307399030455, 0.002595024211399737,\n",
       "                    0.0030538555088334154, 0.003593813663804626,\n",
       "                    0.0042292428743894986, 0.0049770235643321085,\n",
       "                    0.005857020818056662, 0.006892612104349695,\n",
       "                    0.008111308307896872, 0.009545484566618337,\n",
       "                    0.011233240329780276, 0.013219411484660288,\n",
       "                    0.015556761439304723, 0.01830738280295368,\n",
       "                    0.021544346900318822, 0.025353644939701114,\n",
       "                    0.029836472402833374, 0.03511191734215131,\n",
       "                    0.04132012400115335, 0.04862601580065353,\n",
       "                    0.057223676593502144, 0.06734150657750822,\n",
       "                    0.0792482898353917, 0.093260334688322,\n",
       "                    0.10974987654930557, 0.1291549665014884,\n",
       "                    0.1519911082952933, 0.1788649529057435,\n",
       "                    0.21049041445120198, 0.2477076355991709,\n",
       "                    0.2915053062825176, 0.34304692863149155,\n",
       "                    0.40370172585965536, 0.4750810162102793,\n",
       "                    0.5590810182512223, 0.6579332246575675,\n",
       "                    0.774263682681127, 0.9111627561154887,\n",
       "                    1.072267222010323, 1.261856883066021,\n",
       "                    1.4849682622544635, 1.747528400007683,\n",
       "                    2.0565123083486516, 2.4201282647943834,\n",
       "                    2.848035868435799, 3.351602650938841,\n",
       "                    3.944206059437656, 4.641588833612772,\n",
       "                    5.462277217684337, 6.4280731172843195,\n",
       "                    7.56463327554629, 8.902150854450374, 10.47615752789664,\n",
       "                    12.32846739442066, 14.508287784959402,\n",
       "                    17.073526474706888, 20.09233002565046,\n",
       "                    23.644894126454073, 27.825594022071257,\n",
       "                    32.74549162877725, 38.53528593710527,\n",
       "                    45.34878508128582, 53.36699231206302,\n",
       "                    62.80291441834247, 73.90722033525775,\n",
       "                    86.97490026177834, 102.35310218990247,\n",
       "                    120.45035402587811, 141.74741629268047,\n",
       "                    166.81005372000593, 196.30406500402685,\n",
       "                    231.01297000831582, 271.85882427329403,\n",
       "                    319.92671377973846, 376.49358067924635,\n",
       "                    443.06214575838777, 521.4008287999684,\n",
       "                    613.5907273413163, 722.0809018385456,\n",
       "                    849.7534359086438, 1000.0],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'alpha': 0.0001},\n",
       "  {'alpha': 0.0001176811952434998},\n",
       "  {'alpha': 0.00013848863713938732},\n",
       "  {'alpha': 0.00016297508346206434},\n",
       "  {'alpha': 0.00019179102616724886},\n",
       "  {'alpha': 0.0002257019719633919},\n",
       "  {'alpha': 0.00026560877829466864},\n",
       "  {'alpha': 0.00031257158496882353},\n",
       "  {'alpha': 0.0003678379771828634},\n",
       "  {'alpha': 0.0004328761281083057},\n",
       "  {'alpha': 0.000509413801481638},\n",
       "  {'alpha': 0.0005994842503189409},\n",
       "  {'alpha': 0.0007054802310718645},\n",
       "  {'alpha': 0.0008302175681319744},\n",
       "  {'alpha': 0.0009770099572992247},\n",
       "  {'alpha': 0.0011497569953977356},\n",
       "  {'alpha': 0.0013530477745798076},\n",
       "  {'alpha': 0.0015922827933410922},\n",
       "  {'alpha': 0.001873817422860383},\n",
       "  {'alpha': 0.0022051307399030455},\n",
       "  {'alpha': 0.002595024211399737},\n",
       "  {'alpha': 0.0030538555088334154},\n",
       "  {'alpha': 0.003593813663804626},\n",
       "  {'alpha': 0.0042292428743894986},\n",
       "  {'alpha': 0.0049770235643321085},\n",
       "  {'alpha': 0.005857020818056662},\n",
       "  {'alpha': 0.006892612104349695},\n",
       "  {'alpha': 0.008111308307896872},\n",
       "  {'alpha': 0.009545484566618337},\n",
       "  {'alpha': 0.011233240329780276},\n",
       "  {'alpha': 0.013219411484660288},\n",
       "  {'alpha': 0.015556761439304723},\n",
       "  {'alpha': 0.01830738280295368},\n",
       "  {'alpha': 0.021544346900318822},\n",
       "  {'alpha': 0.025353644939701114},\n",
       "  {'alpha': 0.029836472402833374},\n",
       "  {'alpha': 0.03511191734215131},\n",
       "  {'alpha': 0.04132012400115335},\n",
       "  {'alpha': 0.04862601580065353},\n",
       "  {'alpha': 0.057223676593502144},\n",
       "  {'alpha': 0.06734150657750822},\n",
       "  {'alpha': 0.0792482898353917},\n",
       "  {'alpha': 0.093260334688322},\n",
       "  {'alpha': 0.10974987654930557},\n",
       "  {'alpha': 0.1291549665014884},\n",
       "  {'alpha': 0.1519911082952933},\n",
       "  {'alpha': 0.1788649529057435},\n",
       "  {'alpha': 0.21049041445120198},\n",
       "  {'alpha': 0.2477076355991709},\n",
       "  {'alpha': 0.2915053062825176},\n",
       "  {'alpha': 0.34304692863149155},\n",
       "  {'alpha': 0.40370172585965536},\n",
       "  {'alpha': 0.4750810162102793},\n",
       "  {'alpha': 0.5590810182512223},\n",
       "  {'alpha': 0.6579332246575675},\n",
       "  {'alpha': 0.774263682681127},\n",
       "  {'alpha': 0.9111627561154887},\n",
       "  {'alpha': 1.072267222010323},\n",
       "  {'alpha': 1.261856883066021},\n",
       "  {'alpha': 1.4849682622544635},\n",
       "  {'alpha': 1.747528400007683},\n",
       "  {'alpha': 2.0565123083486516},\n",
       "  {'alpha': 2.4201282647943834},\n",
       "  {'alpha': 2.848035868435799},\n",
       "  {'alpha': 3.351602650938841},\n",
       "  {'alpha': 3.944206059437656},\n",
       "  {'alpha': 4.641588833612772},\n",
       "  {'alpha': 5.462277217684337},\n",
       "  {'alpha': 6.4280731172843195},\n",
       "  {'alpha': 7.56463327554629},\n",
       "  {'alpha': 8.902150854450374},\n",
       "  {'alpha': 10.47615752789664},\n",
       "  {'alpha': 12.32846739442066},\n",
       "  {'alpha': 14.508287784959402},\n",
       "  {'alpha': 17.073526474706888},\n",
       "  {'alpha': 20.09233002565046},\n",
       "  {'alpha': 23.644894126454073},\n",
       "  {'alpha': 27.825594022071257},\n",
       "  {'alpha': 32.74549162877725},\n",
       "  {'alpha': 38.53528593710527},\n",
       "  {'alpha': 45.34878508128582},\n",
       "  {'alpha': 53.36699231206302},\n",
       "  {'alpha': 62.80291441834247},\n",
       "  {'alpha': 73.90722033525775},\n",
       "  {'alpha': 86.97490026177834},\n",
       "  {'alpha': 102.35310218990247},\n",
       "  {'alpha': 120.45035402587811},\n",
       "  {'alpha': 141.74741629268047},\n",
       "  {'alpha': 166.81005372000593},\n",
       "  {'alpha': 196.30406500402685},\n",
       "  {'alpha': 231.01297000831582},\n",
       "  {'alpha': 271.85882427329403},\n",
       "  {'alpha': 319.92671377973846},\n",
       "  {'alpha': 376.49358067924635},\n",
       "  {'alpha': 443.06214575838777},\n",
       "  {'alpha': 521.4008287999684},\n",
       "  {'alpha': 613.5907273413163},\n",
       "  {'alpha': 722.0809018385456},\n",
       "  {'alpha': 849.7534359086438},\n",
       "  {'alpha': 1000.0}],\n",
       " 'split0_test_score': array([-3.52991302, -3.52991265, -3.52991222, -3.52991171, -3.52991111,\n",
       "        -3.52991041, -3.52990958, -3.5299086 , -3.52990746, -3.52990611,\n",
       "        -3.52990452, -3.52990265, -3.52990045, -3.52989786, -3.52989481,\n",
       "        -3.52989122, -3.529887  , -3.52988204, -3.52987619, -3.52986931,\n",
       "        -3.52986122, -3.52985169, -3.52984049, -3.52982729, -3.52981177,\n",
       "        -3.52979351, -3.52977201, -3.52974672, -3.52971696, -3.52968193,\n",
       "        -3.52964072, -3.52959223, -3.52953516, -3.52946802, -3.52938903,\n",
       "        -3.52929608, -3.52918672, -3.52905807, -3.52890672, -3.52872868,\n",
       "        -3.52851925, -3.52827293, -3.52798324, -3.52764259, -3.52724205,\n",
       "        -3.52677119, -3.52621775, -3.52556738, -3.52480331, -3.52390593,\n",
       "        -3.52285233, -3.52161584, -3.52016539, -3.51846493, -3.51647267,\n",
       "        -3.51414034, -3.51141238, -3.50822505, -3.50450561, -3.5001715 ,\n",
       "        -3.49512969, -3.48927617, -3.48249585, -3.47466297, -3.46564221,\n",
       "        -3.45529087, -3.44346235, -3.4300114 , -3.41480153, -3.39771497,\n",
       "        -3.37866578, -3.35761615, -3.33459637, -3.30972787, -3.28324896,\n",
       "        -3.25554158, -3.22715694, -3.19883679, -3.17152641, -3.14637496,\n",
       "        -3.12471933, -3.10804844, -3.09794724, -3.09602206, -3.1038114 ,\n",
       "        -3.1226887 , -3.15376444, -3.19779539, -3.25510827, -3.3255443 ,\n",
       "        -3.40843059, -3.50258322, -3.60634645, -3.71766971, -3.83422133,\n",
       "        -3.95353269, -4.07316192, -4.19086125, -4.30472945, -4.41333059]),\n",
       " 'split1_test_score': array([-5.10378223, -5.10378174, -5.10378117, -5.1037805 , -5.1037797 ,\n",
       "        -5.10377877, -5.10377767, -5.10377638, -5.10377486, -5.10377307,\n",
       "        -5.10377097, -5.10376849, -5.10376557, -5.10376214, -5.1037581 ,\n",
       "        -5.10375335, -5.10374776, -5.10374118, -5.10373344, -5.10372432,\n",
       "        -5.1037136 , -5.10370098, -5.10368613, -5.10366866, -5.1036481 ,\n",
       "        -5.1036239 , -5.10359542, -5.10356192, -5.10352249, -5.1034761 ,\n",
       "        -5.10342151, -5.10335728, -5.1032817 , -5.10319279, -5.10308817,\n",
       "        -5.1029651 , -5.10282031, -5.10264999, -5.10244965, -5.10221403,\n",
       "        -5.10193692, -5.10161107, -5.10122795, -5.10077757, -5.10024823,\n",
       "        -5.09962622, -5.09889551, -5.09803737, -5.09702996, -5.0958478 ,\n",
       "        -5.09446131, -5.09283615, -5.09093258, -5.08870481, -5.08610019,\n",
       "        -5.0830586 , -5.07951167, -5.0753823 , -5.07058429, -5.06502238,\n",
       "        -5.05859284, -5.05118481, -5.04268288, -5.03297114, -5.02193921,\n",
       "        -5.00949089, -4.99555561, -4.98010332, -4.96316255, -4.94484121,\n",
       "        -4.92534875, -4.90501733, -4.88431857, -4.86387188, -4.84443999,\n",
       "        -4.82690891, -4.8122517 , -4.80147951, -4.79558736, -4.7955052 ,\n",
       "        -4.8020651 , -4.81599261, -4.8379237 , -4.86844179, -4.90812278,\n",
       "        -4.95757308, -5.01744697, -5.08843421, -5.17121571, -5.26639223,\n",
       "        -5.37439701, -5.49540704, -5.62926869, -5.77545179, -5.93304133,\n",
       "        -6.10076942, -6.27708257, -6.46023282, -6.64837715, -6.83966947]),\n",
       " 'split2_test_score': array([-5.75101108, -5.75101094, -5.75101076, -5.75101056, -5.75101032,\n",
       "        -5.75101004, -5.75100971, -5.75100932, -5.75100887, -5.75100833,\n",
       "        -5.75100769, -5.75100695, -5.75100607, -5.75100504, -5.75100383,\n",
       "        -5.7510024 , -5.75100072, -5.75099874, -5.75099641, -5.75099367,\n",
       "        -5.75099044, -5.75098665, -5.75098218, -5.75097693, -5.75097074,\n",
       "        -5.75096347, -5.75095491, -5.75094483, -5.75093298, -5.75091903,\n",
       "        -5.75090263, -5.75088332, -5.75086061, -5.75083389, -5.75080246,\n",
       "        -5.75076549, -5.75072202, -5.75067088, -5.75061076, -5.75054007,\n",
       "        -5.75045698, -5.75035932, -5.75024456, -5.75010976, -5.74995147,\n",
       "        -5.74976565, -5.74954761, -5.74929191, -5.74899222, -5.74864123,\n",
       "        -5.74823051, -5.74775037, -5.74718974, -5.74653604, -5.74577507,\n",
       "        -5.74489095, -5.74386611, -5.74268134, -5.7413161 , -5.73974886,\n",
       "        -5.73795786, -5.73592214, -5.73362314, -5.73104687, -5.72818689,\n",
       "        -5.72504818, -5.72165226, -5.71804341, -5.71429638, -5.7105255 ,\n",
       "        -5.70689516, -5.7036315 , -5.7010351 , -5.69949408, -5.69949724,\n",
       "        -5.70164625, -5.70666624, -5.71541346, -5.72887894, -5.7481865 ,\n",
       "        -5.77458393, -5.80942564, -5.85414574, -5.91022048, -5.9791196 ,\n",
       "        -6.06224643, -6.16086764, -6.27603502, -6.40850357, -6.55865324,\n",
       "        -6.72642358, -6.91127247, -7.11216871, -7.32762461, -7.55576859,\n",
       "        -7.79444988, -8.04136114, -8.29416065, -8.55057609, -8.80847663]),\n",
       " 'split3_test_score': array([-8.98678727, -8.98678702, -8.98678672, -8.98678637, -8.98678596,\n",
       "        -8.98678547, -8.9867849 , -8.98678423, -8.98678344, -8.98678251,\n",
       "        -8.98678142, -8.98678013, -8.98677861, -8.98677683, -8.98677473,\n",
       "        -8.98677226, -8.98676935, -8.98676593, -8.98676191, -8.98675717,\n",
       "        -8.98675159, -8.98674503, -8.98673731, -8.98672823, -8.98671754,\n",
       "        -8.98670495, -8.98669015, -8.98667272, -8.98665222, -8.98662809,\n",
       "        -8.98659969, -8.98656628, -8.98652696, -8.98648068, -8.98642623,\n",
       "        -8.98636215, -8.98628674, -8.98619801, -8.98609359, -8.98597073,\n",
       "        -8.98582615, -8.98565603, -8.98545586, -8.98522033, -8.98494322,\n",
       "        -8.98461717, -8.98423359, -8.98378231, -8.98325145, -8.982627  ,\n",
       "        -8.98189251, -8.98102869, -8.98001289, -8.97881852, -8.97741442,\n",
       "        -8.97576412, -8.97382489, -8.97154685, -8.96887173, -8.96573172,\n",
       "        -8.96204796, -8.95772912, -8.95266975, -8.94674868, -8.93982746,\n",
       "        -8.93174907, -8.922337  , -8.91139515, -8.89870876, -8.88404702,\n",
       "        -8.86716767, -8.84782432, -8.82577678, -8.80080469, -8.77272433,\n",
       "        -8.74140788, -8.70680392, -8.66895742, -8.62802702, -8.58429745,\n",
       "        -8.53818544, -8.49023818, -8.44112474, -8.39162135, -8.34259242,\n",
       "        -8.29496864, -8.24972277, -8.20784271, -8.17030063, -8.13801703,\n",
       "        -8.11182039, -8.09240534, -8.08029547, -8.07581916, -8.07910634,\n",
       "        -8.09011125, -8.10866003, -8.13451464, -8.16743818, -8.20724378]),\n",
       " 'split4_test_score': array([-5.77178775, -5.77178664, -5.77178533, -5.77178379, -5.77178197,\n",
       "        -5.77177984, -5.77177732, -5.77177437, -5.77177089, -5.77176679,\n",
       "        -5.77176197, -5.7717563 , -5.77174962, -5.77174177, -5.77173252,\n",
       "        -5.77172164, -5.77170884, -5.77169378, -5.77167605, -5.77165519,\n",
       "        -5.77163064, -5.77160175, -5.77156776, -5.77152775, -5.77148068,\n",
       "        -5.77142529, -5.77136011, -5.77128341, -5.77119317, -5.77108699,\n",
       "        -5.77096205, -5.77081506, -5.77064212, -5.77043866, -5.77019931,\n",
       "        -5.76991775, -5.76958658, -5.76919707, -5.76873899, -5.76820035,\n",
       "        -5.76756705, -5.76682259, -5.76594761, -5.76491948, -5.7637117 ,\n",
       "        -5.76229332, -5.76062823, -5.75867436, -5.75638276, -5.75369665,\n",
       "        -5.75055029, -5.7468678 , -5.74256191, -5.73753268, -5.7316662 ,\n",
       "        -5.72483342, -5.7168892 , -5.70767165, -5.69700211, -5.68468583,\n",
       "        -5.67051385, -5.65426623, -5.63571712, -5.61464198, -5.5908272 ,\n",
       "        -5.56408224, -5.53425402, -5.50124317, -5.46502075, -5.42564414,\n",
       "        -5.38326956, -5.33815921, -5.29068032, -5.2412946 , -5.19053764,\n",
       "        -5.13898917, -5.08723748, -5.03584237, -4.98530248, -4.93603253,\n",
       "        -4.88835552, -4.84251315, -4.79869544, -4.75708884, -4.71793931,\n",
       "        -4.6816256 , -4.64873596, -4.62014017, -4.59704812, -4.58104514,\n",
       "        -4.57409462, -4.57849925, -4.59681464, -4.63171301, -4.68580142,\n",
       "        -4.76140678, -4.8603489 , -4.98372901, -5.1317627 , -5.3036816 ]),\n",
       " 'mean_test_score': array([-5.82865627, -5.8286558 , -5.82865524, -5.82865458, -5.82865381,\n",
       "        -5.82865291, -5.82865184, -5.82865058, -5.8286491 , -5.82864736,\n",
       "        -5.82864531, -5.8286429 , -5.82864007, -5.82863673, -5.8286328 ,\n",
       "        -5.82862818, -5.82862273, -5.82861633, -5.8286088 , -5.82859993,\n",
       "        -5.8285895 , -5.82857722, -5.82856277, -5.82854577, -5.82852577,\n",
       "        -5.82850222, -5.82847452, -5.82844192, -5.82840356, -5.82835843,\n",
       "        -5.82830532, -5.82824283, -5.82816931, -5.82808281, -5.82798104,\n",
       "        -5.82786131, -5.82772047, -5.8275548 , -5.82735994, -5.82713077,\n",
       "        -5.82686127, -5.82654439, -5.82617185, -5.82573395, -5.82521933,\n",
       "        -5.82461471, -5.82390454, -5.82307067, -5.82209194, -5.82094372,\n",
       "        -5.81959739, -5.81801977, -5.8161725 , -5.81401139, -5.81148571,\n",
       "        -5.80853749, -5.80510085, -5.80110144, -5.79645597, -5.79107206,\n",
       "        -5.78484844, -5.77767569, -5.76943775, -5.76001433, -5.74928459,\n",
       "        -5.73713225, -5.72345225, -5.70815929, -5.691198  , -5.67255457,\n",
       "        -5.65226939, -5.6304497 , -5.60728143, -5.58303862, -5.55808963,\n",
       "        -5.53289876, -5.50802325, -5.48410591, -5.46186444, -5.44207933,\n",
       "        -5.42558186, -5.4132436 , -5.40596737, -5.4046789 , -5.4103171 ,\n",
       "        -5.42382049, -5.44610756, -5.4780495 , -5.52043526, -5.57393039,\n",
       "        -5.63903324, -5.71603347, -5.80497879, -5.90565566, -6.0175878 ,\n",
       "        -6.140054  , -6.27212291, -6.41269967, -6.56057671, -6.71448042]),\n",
       " 'std_test_score': array([1.77722914, 1.7772292 , 1.77722926, 1.77722933, 1.77722942,\n",
       "        1.77722952, 1.77722964, 1.77722978, 1.77722995, 1.77723014,\n",
       "        1.77723037, 1.77723065, 1.77723096, 1.77723134, 1.77723178,\n",
       "        1.7772323 , 1.77723291, 1.77723363, 1.77723448, 1.77723548,\n",
       "        1.77723665, 1.77723803, 1.77723965, 1.77724156, 1.77724381,\n",
       "        1.77724646, 1.77724957, 1.77725324, 1.77725755, 1.77726262,\n",
       "        1.77726859, 1.77727562, 1.77728388, 1.77729361, 1.77730505,\n",
       "        1.77731851, 1.77733434, 1.77735297, 1.77737488, 1.77740065,\n",
       "        1.77743095, 1.77746658, 1.77750848, 1.77755772, 1.7776156 ,\n",
       "        1.77768361, 1.77776349, 1.7778573 , 1.77796741, 1.7780966 ,\n",
       "        1.77824809, 1.7784256 , 1.77863345, 1.77887657, 1.77916063,\n",
       "        1.77949204, 1.77987806, 1.78032674, 1.78084696, 1.78144826,\n",
       "        1.78214068, 1.78293435, 1.78383888, 1.7848625 , 1.78601072,\n",
       "        1.78728459, 1.7886783 , 1.79017626, 1.79174951, 1.79335174,\n",
       "        1.7949152 , 1.79634703, 1.79752669, 1.79830542, 1.79850842,\n",
       "        1.79794041, 1.79639454, 1.7936642 , 1.78955676, 1.78390787,\n",
       "        1.77659546, 1.76755326, 1.75678421, 1.74437479, 1.7305103 ,\n",
       "        1.71548983, 1.69973695, 1.68380041, 1.66833787, 1.6540774 ,\n",
       "        1.6417557 , 1.63203932, 1.62544334, 1.62226821, 1.62257518,\n",
       "        1.62621253, 1.632889  , 1.64227432, 1.65409592, 1.66820135]),\n",
       " 'rank_test_score': array([ 93,  92,  91,  90,  89,  88,  87,  86,  85,  84,  83,  82,  81,\n",
       "         80,  79,  78,  77,  76,  75,  74,  73,  72,  71,  70,  69,  68,\n",
       "         67,  66,  65,  64,  63,  62,  61,  60,  59,  58,  57,  56,  55,\n",
       "         54,  53,  52,  51,  50,  49,  48,  47,  46,  45,  44,  43,  42,\n",
       "         41,  40,  39,  38,  37,  35,  34,  33,  32,  31,  30,  29,  28,\n",
       "         27,  26,  24,  23,  22,  21,  19,  18,  17,  15,  14,  12,  11,\n",
       "          9,   7,   6,   4,   2,   1,   3,   5,   8,  10,  13,  16,  20,\n",
       "         25,  36,  94,  95,  96,  97,  98,  99, 100])}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = data_X#[best_features]\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "clsf = Ridge()\n",
    "\n",
    "parameters = {\n",
    "    'alpha': np.logspace(-4, 3, 100), \n",
    "}\n",
    "\n",
    "srch = GridSearchCV(\n",
    "    estimator=clsf,\n",
    "    scoring='neg_root_mean_squared_error', \n",
    "    param_grid=parameters,\n",
    "    \n",
    "#     # Cross-validation splits\n",
    "    cv = 5\n",
    ")\n",
    "\n",
    "srch.fit(X, y)\n",
    "\n",
    "res = srch.cv_results_\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best value of alpha for Ridge is 73.907\n",
      "With a RMSE of 5.405\n"
     ]
    }
   ],
   "source": [
    "best_alpha_index = res['rank_test_score'].argmin()\n",
    "\n",
    "print(f\"The best value of alpha for Ridge is {res['params'][best_alpha_index]['alpha']:.3f}\")\n",
    "print(f\"With a RMSE of {-res['mean_test_score'][best_alpha_index]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x283fff827c0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x1080 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ids = np.arange(10,50)\n",
    "plt.figure(figsize=(15,15))\n",
    "\n",
    "plt.xscale('log',base=10)\n",
    "arr = np.array(list(map(lambda x: x['alpha'], res['params'])))\n",
    "vals = res['mean_test_score']\n",
    "\n",
    "plt.plot(arr, vals)\n",
    "plt.scatter([res['params'][best_alpha_index]['alpha']], [res['mean_test_score'][best_alpha_index]], c='r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Gradient descent\n",
    "\n",
    "#### 6. [3.5 points] \n",
    "**Implement a linear regression model for the MSE loss function, trained by gradient descent.**\n",
    "\n",
    "All calculations must be vectorized, and python loops can only be used for gradient descent iterations. As a stop criterion, you must use (simultaneously):\n",
    "\n",
    "* checking for the Euclidean norm of the weight difference on two adjacent iterations (for example, less than some small number of the order of $10^{-6}$, set by the `tolerance` parameter);\n",
    "* reaching the maximum number of iterations (for example, 10000, set by the `max_iter` parameter).\n",
    "\n",
    "You need to implement:\n",
    "\n",
    "* Full gradient descent:\n",
    "\n",
    "$$\n",
    "w_{k + 1} = w_{k} - \\eta_{k} \\nabla_{w} Q(w_{k}).\n",
    "$$\n",
    "\n",
    "* Stochastic Gradient Descent:\n",
    "\n",
    "$$\n",
    "w_{k + 1} = w_{k} - \\eta_{k} \\nabla_{w} q_{i_{k}}(w_{k}).\n",
    "$$\n",
    "\n",
    "$\\nabla_{w} q_{i_{k}}(w_{k}) \\, $ is the estimate of the gradient over the batch of objects selected randomly.\n",
    "\n",
    "* Momentum method:\n",
    "\n",
    "$$\n",
    "h_0 = 0, \\\\\n",
    "h_{k + 1} = \\alpha h_{k} + \\eta_k \\nabla_{w} q_{i_{k}} (w_{k}), \\\\\n",
    "w_{k + 1} = w_{k} - h_{k + 1}.\n",
    "$$\n",
    "\n",
    "Exponentially weighed averages can provide a better estimate which is closer to the actual gradient.\n",
    "\n",
    "\n",
    "To make sure that the optimization process really converges, we will use the `loss_history` class attribute. After calling the `fit` method, it should contain the values of the loss function for all iterations, starting from the first one (before the first step on the anti-gradient).\n",
    "\n",
    "You need to initialize the weights with a zero or random (from a normal distribution) vector. The following is a template class that needs to contain the code implementing all variations of the models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculations \n",
    "\n",
    "$$\n",
    "\\nabla_{w} Q(w_{k}) = \\nabla_{w} (y-Xw)^{T} (y-Xw) = \\nabla_{w} [y^{T} y - y^{T}Xw - w^{T}X^{T}y + w^{T}X^{T}Xw] =\n",
    "$$\n",
    "\n",
    "$$\n",
    "=\\nabla_{w} [-(y^{T}X)^{T} -X^{T}y + (X^{T}X + (X^{T}X)^{T})w] = -2X^{T}y + 2X^{T}Xw = 2X^{T}(Xw-y).\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator\n",
    "\n",
    "class LinReg(BaseEstimator):\n",
    "    def __init__(self, delta=0.2, gd_type='Momentum', \n",
    "                 tolerance=1e-4, max_iter=1000, w0=None, eta=1e-2, alpha=1e-3):\n",
    "        \"\"\"\n",
    "        gd_type: str\n",
    "            'GradientDescent', 'StochasticDescent', 'Momentum'\n",
    "        delta: float\n",
    "            proportion of object in a batch (fot stochastic GD)\n",
    "        tolerance: float\n",
    "            for stopping gradient descent\n",
    "        max_iter: int\n",
    "            maximum number of steps in gradient descent\n",
    "        w0: np.array of shape (d)\n",
    "            init weights\n",
    "        eta: float\n",
    "            learning rate\n",
    "        alpha: float\n",
    "            momentum coefficient\n",
    "        \"\"\"\n",
    "        \n",
    "        self.gd_type = gd_type\n",
    "        self.delta = delta\n",
    "        self.tolerance = tolerance\n",
    "        self.max_iter = max_iter\n",
    "        self.w0 = w0\n",
    "        self.alpha = alpha\n",
    "        self.w = None\n",
    "        self.eta = eta\n",
    "        self.loss_history = None # list of loss function values at each training iteration\n",
    "    \n",
    "    def __full_iter(self, X, y):\n",
    "        self.w -= self.eta * self.calc_gradient(X, y)\n",
    "    \n",
    "    def __stochastic_iter(self, X, y):\n",
    "        randind = np.random.choice(len(X), self.batch_size, replace=False)\n",
    "        rand_X, rand_y = X[randind], y[randind]\n",
    "        self.w -= self.eta * self.calc_gradient(rand_X, rand_y)\n",
    "        \n",
    "    def __momentum_iter(self, X, y):\n",
    "        randind = np.random.choice(len(X), self.batch_size, replace=False)\n",
    "        rand_X, rand_y = X[randind], y[randind]\n",
    "        \n",
    "        dh = self.eta * self.calc_gradient(rand_X, rand_y)\n",
    "        \n",
    "        self.h = self.alpha * self.h + dh\n",
    "        \n",
    "        self.w -= self.h\n",
    "        \n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        X: np.array of shape (l, d)\n",
    "        y: np.array of shape (l)\n",
    "        ---\n",
    "        output: self\n",
    "        \"\"\"\n",
    "        \n",
    "        iterations = 0\n",
    "        \n",
    "        X = np.column_stack([X, np.ones(X.shape[0])])\n",
    "        \n",
    "        if self.w0 is None:\n",
    "            self.w = np.zeros(X.shape[1])\n",
    "            self.w[-1] = np.mean(y)\n",
    "        else:\n",
    "            self.w = np.array(self.w0)\n",
    "        \n",
    "        if self.gd_type == 'Momentum':\n",
    "            self.h = np.zeros(*self.w.shape)\n",
    "            \n",
    "        if self.gd_type != 'GradientDescent':\n",
    "            self.batch_size = int(self.delta * X.shape[0])\n",
    "            assert self.batch_size > 0, 'Batch size is too small.'\n",
    "        \n",
    "        \n",
    "        self.loss_history = [self.calc_loss(X, y)]\n",
    "        \n",
    "        iter_function = {\n",
    "            'GradientDescent' :   self.__full_iter,\n",
    "            'StochasticDescent' : self.__stochastic_iter,\n",
    "            'Momentum' :          self.__momentum_iter\n",
    "        } [self.gd_type]\n",
    "        \n",
    "        while iterations < self.max_iter:\n",
    "            \n",
    "            prev_w = np.array(self.w)\n",
    "            \n",
    "            iter_function(X, y)\n",
    "            \n",
    "            if not np.all(np.isfinite(self.w)):\n",
    "                self.w = prev_w\n",
    "                return self\n",
    "            \n",
    "            self.loss_history.append(self.calc_loss(X, y))\n",
    "            \n",
    "            if np.sqrt(np.sum((prev_w - self.w)**2)) < self.tolerance:\n",
    "                break\n",
    "            \n",
    "            iterations += 1\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        if self.w is None:\n",
    "            raise Exception('Not trained yet')\n",
    "        X = np.column_stack([X, np.ones(X.shape[0])])\n",
    "        return X.dot(self.w)\n",
    "    \n",
    "    def __predict(self, X):\n",
    "        return X.dot(self.w)\n",
    "    \n",
    "    def calc_gradient(self, X, y):\n",
    "        \"\"\"\n",
    "        X: np.array of shape (l, d) (l can be equal to 1 if stochastic)\n",
    "        y: np.array of shape (l)\n",
    "        ---\n",
    "        output: np.array of shape (d)\n",
    "        \"\"\"\n",
    "        return 2*X.T.dot(self.__predict(X) - y)\n",
    "\n",
    "    def calc_loss(self, X, y):\n",
    "        \"\"\"\n",
    "        X: np.array of shape (l, d)\n",
    "        y: np.array of shape (l)\n",
    "        ---\n",
    "        output: float \n",
    "        \"\"\" \n",
    "        return np.mean((self.__predict(X) - y)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. [1 points] \n",
    "Train and validate \"hand-written\" model (simple linear regression) on the same data, and compare the quality with the Sklearn or StatsModels methods. Investigate the effect of the `max_iter` and `alpha` parameters on the optimization process. Is it consistent with your expectations?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Max_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X = data_X\n",
    "\n",
    "clsf = LinReg(gd_type='GradientDescent')\n",
    "\n",
    "max_iter_parameters = {\n",
    "    'eta': np.logspace(-8, -4, 25), \n",
    "    'max_iter': np.logspace(3, 5, 5)\n",
    "}\n",
    "\n",
    "max_iter_srch = GridSearchCV(\n",
    "    estimator=clsf,\n",
    "    scoring='neg_root_mean_squared_error', \n",
    "    param_grid=max_iter_parameters,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "max_iter_srch.fit(X, y)\n",
    "\n",
    "max_iter_srch.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(srch_obj, p1, p2, tol='.2f', is_log=True, get_dataframe=False, transpose=False):\n",
    "    \n",
    "    parameters = srch_obj.param_grid\n",
    "    res = srch_obj.cv_results_\n",
    "    \n",
    "#     if transpose:\n",
    "#         p1, p2 = p2, p1\n",
    "    \n",
    "    grid_param_1 = parameters[p1]\n",
    "    grid_param_2 = parameters[p2]\n",
    "\n",
    "    scores_mean = res['mean_test_score']\n",
    "    \n",
    "    if not transpose:\n",
    "        scores_mean = np.array(scores_mean).reshape(len(grid_param_1), len(grid_param_2)).T\n",
    "    else:\n",
    "        scores_mean = np.array(scores_mean).reshape(len(grid_param_2), len(grid_param_1))\n",
    "    \n",
    "    plt.figure(figsize=(16,10))\n",
    "    \n",
    "    if is_log:\n",
    "        plt.xscale('log',base=10) \n",
    "\n",
    "    d = {'X':grid_param_1}\n",
    "        \n",
    "    for idx, val in enumerate(grid_param_2):\n",
    "        vals = scores_mean[idx,:]\n",
    "        title = f'{p2}: {f\"{{0{tol}}}\".format(val)}'\n",
    "        \n",
    "        plt.plot(grid_param_1, vals, '-o', label=title)\n",
    "        if get_dataframe:\n",
    "            d[title] = vals\n",
    "            \n",
    "    plt.title(\"Grid Search Results\", fontsize=16)\n",
    "    plt.xlabel(p1, fontsize=16)\n",
    "    plt.ylabel('Average neg_root_mean_sqaured_error', fontsize=16)\n",
    "    plt.legend(loc=\"best\", fontsize=16)\n",
    "    \n",
    "    if get_dataframe:\n",
    "        return pd.DataFrame(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a plot of negative loss vs. eta for different values `max_iter`. The values are sampled to be equaliy spaced in a log scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "p1 = 'eta'\n",
    "p2 = 'max_iter'\n",
    "\n",
    "df = plot_results(max_iter_srch, p1, p2, get_dataframe=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The graphs loss vs. eta for different values of `max_iter` seem to match each other exactly with some extra offset.\n",
    "\n",
    "Let's plot the graphs for max_iter `1000`, `3162` and `10000` with added offsets (the x-axis is logorithmic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "plt.xscale('log', base=10) \n",
    "\n",
    "plt.plot(df.X * 1000.0, df['max_iter: 1000.00'])\n",
    "plt.plot(df.X * 3162.28, df['max_iter: 3162.28'])\n",
    "plt.plot(df.X * 10000.0, df['max_iter: 10000.00'])\n",
    "\n",
    "# Do not match the pattern\n",
    "plt.plot(df.X * 31622.78, df['max_iter: 31622.78'], '--')\n",
    "plt.plot(df.X * 100000.0, df['max_iter: 100000.00'], '--')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Conclusion```\n",
    "\n",
    "As we can see, for `max_iter` of `1000`, `3162` and `10000` the graphs are basically the same after adding some offset. However, for `31622` and `100000` the pattern breaks. Additionally, on the original diagram (Grid Search Results),\n",
    "the `loss vs. eta` graphs for `31622` and `100000` are equal.\n",
    "\n",
    "So it seems like there is a critical point, after which increasing `max_iter` has no effect. This value is somewhere between `10000` and `31622`. \n",
    "\n",
    "Otherwise, as long as the value of `max_iter` x `eta` stays constant, the loss stays roughly the same.\n",
    "\n",
    "This goes against my expectations, since I thought that increasing max_iter would increase the quality of the model.\n",
    "\n",
    "The quality of this model with the best params seems to match and even exceed the quality from sklearn (neg_rmse ~= -5.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "np.mean(cross_val_score(LinearRegression(), X, y, scoring='neg_root_mean_squared_error'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best value for `max_iter` x `eta`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_iter_best_val = df.X[df['max_iter: 1000.00'].argmax()] * 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_iter_best_val "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X = data_X\n",
    "\n",
    "clsf = LinReg(delta=1.0, max_iter=1000)\n",
    "\n",
    "parameters = {\n",
    "    'alpha': 1 - np.logspace(-1.3, -0.3, 5),\n",
    "    'eta': np.logspace(-9, -5, 15),\n",
    "}\n",
    "\n",
    "alpha_srch = GridSearchCV(\n",
    "    estimator=clsf,\n",
    "    scoring='neg_root_mean_squared_error', \n",
    "    param_grid=parameters,\n",
    ")\n",
    "\n",
    "alpha_srch.fit(X, y)\n",
    "\n",
    "alpha_srch.cv_results_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a plot of negative loss vs. eta for different values `alpha`. The values are sampled such that `1 - alpha` is  equaliy spaced in a log scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "p1 = 'eta'\n",
    "p2 = 'alpha'\n",
    "\n",
    "df = plot_results(alpha_srch, p1, p2, tol=':.2f', is_log=True, get_dataframe=True, transpose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The graphs loss vs. eta for different values of `alpha` seem to match each other exactly with some extra offset.\n",
    "\n",
    "Let's plot these graphs with some added offsets (the x-axis is logorithmic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "one_minus_alpha = np.logspace(-1.3, -0.3, 5)\n",
    "\n",
    "plt.xscale('log', base=10) \n",
    "\n",
    "cols = list(df.columns)[1:]\n",
    "\n",
    "for i in range(len(cols)):\n",
    "    plt.plot(df.X / one_minus_alpha[i], df[cols[i]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Conclusion```\n",
    "\n",
    "All the graphs seem to match after we divided the x scale by the value of `1 - alpha` corresponding to the graph.\n",
    "\n",
    "It seems that as long as `eta / (1 - alpha)` stays constant, the loss stays roughly the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To sum up, both optimal `max_iter` and `alpha` depend on `eta`, and the best loss is reached when  \n",
    "\n",
    "`eta / (1 - alpha)` = 1.4359617019622137e-05\n",
    "\n",
    "and `max_iter x eta` = 0.004641588833612773"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_best_val = df.X[df['alpha: 0.95'].argmax()] / one_minus_alpha[0]\n",
    "alpha_best_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_iter_best_val "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. [1 points] \n",
    "Plot graphs (on the same picture) of the dependence of the loss function value on the iteration number for Full GD, SGD and Momentum. Draw conclusions about the rate of convergence of various modifications of gradient descent.\n",
    "\n",
    "Don't forget about what *beautiful* graphics should look like!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GD_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X = data_X\n",
    "\n",
    "clsf = LinReg(delta = 0.4, eta=0.000004641588833612773, alpha=.8)\n",
    "\n",
    "type_parameters = {\n",
    "    'max_iter': np.logspace(2, 4, 10), \n",
    "    'gd_type': [\n",
    "        'GradientDescent',\n",
    "        'StochasticDescent',\n",
    "        'Momentum'\n",
    "    ]\n",
    "}\n",
    "\n",
    "type_srch = GridSearchCV(\n",
    "    estimator=clsf,\n",
    "    scoring='neg_root_mean_squared_error', \n",
    "    param_grid=type_parameters,\n",
    ")\n",
    "\n",
    "type_srch.fit(X, y)\n",
    "\n",
    "type_srch.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1 = 'max_iter'\n",
    "p2 = 'gd_type'\n",
    "\n",
    "plot_results(type_srch, p1, p2, tol='', transpose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Conclusion```\n",
    "\n",
    "The Momemtum method converges the fastest, \n",
    "\n",
    "The Gradient Descent method is the second,\n",
    "\n",
    "and the Stochastic Descent is the slowest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a8f61be024eba58adef938c9aa1e29e02cb3dece83a5348b1a2dafd16a070453"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
